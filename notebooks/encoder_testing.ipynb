{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "# !pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1259e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from modules.dataset import CityscapesDataset\n",
    "from modules.dataset import StyleGANFaces, scale_width\n",
    "from modules.loss import Pix2PixHDLoss\n",
    "from utils import parse_config, get_lr_lambda, weights_init, freeze_encoder, show_tensor_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-c', '--config', type=str, required=True)\n",
    "    parser.add_argument('-c', '--config', type=str)\n",
    "    parser.add_argument('-r', '--high_res', action='store_true', default=False)\n",
    "    return parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a7ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f35062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02de8b0e",
   "metadata": {},
   "source": [
    "## Create dataloader for generated faces and interpolated result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections.abc import Iterable\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d692738",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StyleGANFaces(\n",
    "    path_A=\"../../dataset/image-to-image/trainA\",\n",
    "    path_B=\"../../dataset/image-to-image/trainB\",\n",
    "    path_AtoB = \"../../dataset/image-to-image/images_AtoB\",\n",
    "    path_BtoA = \"../../dataset/image-to-image/images_BtoA\"\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=True, sampler=None,\n",
    "    num_workers=0, collate_fn=None,\n",
    "    pin_memory=True, drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5968200",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    img_A, img_B, img_AtoB, img_BtoA = batch\n",
    "    print(img_A.shape, img_B.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb91333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tensor_images(img_A)\n",
    "show_tensor_images(img_B)\n",
    "show_tensor_images(img_AtoB)\n",
    "show_tensor_images(img_BtoA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' Implements an encoder with instance-wise average pooling for feature mapping '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        base_channels: int = 64,\n",
    "        n_layers: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        channels = base_channels\n",
    "\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, base_channels, kernel_size=7, padding=0), \n",
    "            nn.InstanceNorm2d(base_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        # Downsampling layers\n",
    "        for i in range(n_layers):\n",
    "            layers += [\n",
    "                nn.Conv2d(channels, channels // 2, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(channels // 2),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            channels //= 2\n",
    "    \n",
    "        # Upsampling layers\n",
    "        for i in range(n_layers):\n",
    "            layers += [\n",
    "                nn.ConvTranspose2d(channels, channels * 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(channels * 2),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            channels *= 2\n",
    "\n",
    "        layers += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(base_channels, out_channels, kernel_size=7, padding=0),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "#     def instancewise_average_pooling(self, x, inst):\n",
    "#         '''\n",
    "#         Applies instance-wise average pooling.\n",
    "\n",
    "#         Given a feature map of size (b, c, h, w), the mean is computed for each b, c\n",
    "#         across all h, w of the same instance\n",
    "#         '''\n",
    "#         x_mean = torch.zeros_like(x)\n",
    "#         classes = torch.unique(inst, return_inverse=False, return_counts=False) # gather all unique classes present\n",
    "\n",
    "#         for i in classes:\n",
    "#             for b in range(x.size(0)):\n",
    "#                 indices = torch.nonzero(inst[b:b+1] == i, as_tuple=False) # get indices of all positions equal to class i\n",
    "#                 for j in range(self.out_channels):\n",
    "#                     x_ins = x[indices[:, 0] + b, indices[:, 1] + j, indices[:, 2], indices[:, 3]]\n",
    "#                     mean_feat = torch.mean(x_ins).expand_as(x_ins)\n",
    "#                     x_mean[indices[:, 0] + b, indices[:, 1] + j, indices[:, 2], indices[:, 3]] = mean_feat\n",
    "\n",
    "#         return x_mean    \n",
    "        \n",
    "\n",
    "    def forward(self, x, inst=None):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b30372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.networks import VGG19\n",
    "import lpips\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd39f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40955cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# main() ###############\n",
    "#\n",
    "\n",
    "args = parse_arguments()\n",
    "args.config = \"notebook_lowres_custom.yml\"\n",
    "\n",
    "with open(args.config, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config = OmegaConf.create(config)\n",
    "    config = parse_config(config)\n",
    "    \n",
    "print(config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# encoder = instantiate(config.encoder).to(device).apply(weights_init)\n",
    "# encoder = Encoder(in_channels=6, out_channels=3, base_channels=128).to(device).apply(weights_init)\n",
    "# encoder =  VGG19().to(device)\n",
    "# encoder = lpips.LPIPS(net='alex', spatial=True).to(device)\n",
    "encoder = lpips.LPIPS(net='vgg', spatial=True).to(device)\n",
    "generator = instantiate(config.generator).to(device).apply(weights_init)\n",
    "discriminator = instantiate(config.discriminator).to(device).apply(weights_init)\n",
    "\n",
    "# summary(encoder, (6, 256, 256))\n",
    "# summary(generator, (9, 256, 256))\n",
    "# summary(discriminator, (9, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee607e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(encoder, (6, 256, 256))\n",
    "encoder = lpips.LPIPS(net='vgg', spatial=True).to(device)\n",
    "x = encoder(img_B.to(device), img_BtoA.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90dff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(img_A.shape)\n",
    "show_tensor_images(x)\n",
    "plt.imshow(x[0,0,...].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_tensor_images(x[0].squeeze()[1:11])\n",
    "for a in x[0]:\n",
    "#     for b in a:\n",
    "    print(a.shape)\n",
    "    show_tensor_images(a[1:3])\n",
    "    break\n",
    "#     print(a.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_enc = encoder(img_B.cuda())\n",
    "# print(b_enc.shape)\n",
    "# print(img_A.shape)\n",
    "\n",
    "# x = torch.cat([img_A.cuda(), b_enc], dim=1)\n",
    "# print(x.shape)\n",
    "\n",
    "# x = generator(x)\n",
    "# print(x.shape)\n",
    "\n",
    "# y = discriminator(x)\n",
    "# print()\n",
    "# for dis in y:\n",
    "#     for b in dis:\n",
    "#         print(b.shape)\n",
    "\n",
    "# # show_tensor_images(b_enc)\n",
    "# # show_tensor_images(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b01a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5941b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.high_res:\n",
    "    g_optimizer = torch.optim.Adam(\n",
    "        list(generator.parameters()), **config.optim,\n",
    "    )\n",
    "else:\n",
    "    g_optimizer = torch.optim.Adam(\n",
    "        list(generator.parameters()) + list(encoder.parameters()), **config.optim,\n",
    "    )\n",
    "d_optimizer = torch.optim.Adam(list(discriminator.parameters()), **config.optim)\n",
    "g_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    g_optimizer,\n",
    "    get_lr_lambda(config.train.epochs, config.train.decay_after),\n",
    ")\n",
    "d_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    d_optimizer,\n",
    "    get_lr_lambda(config.train.epochs, config.train.decay_after),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if config.resume_checkpoint is not None:\n",
    "    state_dict = torch.load(config.resume_checkpoint)\n",
    "\n",
    "    encoder.load_state_dict(state_dict['e_model_dict'])\n",
    "    generator.load_state_dict(state_dict['g_model_dict'])\n",
    "    discriminator.load_state_dict(state_dict['d_model_dict'])\n",
    "    g_optimizer.load_state_dict(state_dict['g_optim_dict'])\n",
    "    d_optimizer.load_state_dict(state_dict['d_optim_dict'])\n",
    "    start_epoch = state_dict['epoch']\n",
    "\n",
    "    msg = 'high-res' if args.high_res else 'low-res'\n",
    "    print(f'Starting {msg} training from checkpoints')\n",
    "\n",
    "elif args.high_res:\n",
    "    state_dict = config.pretrain_checkpoint\n",
    "    if state_dict is not None:\n",
    "        encoder.load_state_dict(torch.load(state_dict['e_model_dict']))\n",
    "        encoder = freeze_encoder(encoder)\n",
    "        generator.g1.load_state_dict(torch.load(state_dict['g_model_dict']))\n",
    "        print('Starting high-res training from pretrained low-res checkpoints')\n",
    "    else:\n",
    "        print('Starting high-res training from scratch (no valid checkpoint detected)')\n",
    "\n",
    "else:\n",
    "    print('Starting low-res training from random initialization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "\n",
    "lpips_loss = lpips.LPIPS(net='alex').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b72297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modules.networks import VGG19\n",
    "\n",
    "device='cuda'\n",
    "lambda1=10. \n",
    "lambda2=10.\n",
    "norm_weight_to_one=True\n",
    "\n",
    "vgg = VGG19().to(device)\n",
    "vgg_weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
    "\n",
    "lambda0 = 1.0\n",
    "# Keep ratio of composite loss, but scale down max to 1.0\n",
    "scale = max(lambda0, lambda1, lambda2) if norm_weight_to_one else 1.0\n",
    "\n",
    "lambda0 = lambda0 / scale\n",
    "lambda1 = lambda1 / scale\n",
    "lambda2 = lambda2 / scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29858686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_loss(x_real, x_fake):\n",
    "    ''' Computes perceptual loss with VGG network from real and fake images '''\n",
    "    vgg_real = vgg(x_real)\n",
    "    vgg_fake = vgg(x_fake)\n",
    "\n",
    "    vgg_loss = 0.0\n",
    "    for real, fake, weight in zip(vgg_real, vgg_fake, vgg_weights):\n",
    "        vgg_loss += weight * F.l1_loss(real.detach(), fake)\n",
    "\n",
    "    return vgg_loss\n",
    "\n",
    "\n",
    "def fm_loss(real_preds, fake_preds):\n",
    "    ''' Computes feature matching loss from nested lists of fake and real outputs from discriminator '''\n",
    "    fm_loss = 0.0\n",
    "    for real_features, fake_features in zip(real_preds, fake_preds):\n",
    "        for real_feature, fake_feature in zip(real_features, fake_features):\n",
    "            fm_loss += F.l1_loss(real_feature.detach(), fake_feature)\n",
    "    \n",
    "    return fm_loss\n",
    "\n",
    "\n",
    "def adv_loss(discriminator_preds, is_real):\n",
    "    ''' Computes adversarial loss from nested list of fakes outputs from discriminator '''\n",
    "    target = torch.ones_like if is_real else torch.zeros_like\n",
    "\n",
    "    adv_loss = 0.0\n",
    "    for preds in discriminator_preds:\n",
    "        pred = preds[-1]\n",
    "        adv_loss += F.mse_loss(pred, target(pred))\n",
    "    \n",
    "    return adv_loss\n",
    "\n",
    "\n",
    "def enc_loss(f_map, img_orig):\n",
    "    return F.l1_loss(f_map, img_orig)\n",
    "\n",
    "\n",
    "def forward_loss(img_A, img_B, img_AtoB, encoder, generator, discriminator):\n",
    "    # Forward call of loss.\n",
    "    #\n",
    "    x_real = img_AtoB\n",
    "\n",
    "#     feature_map = encoder(img_B)\n",
    "    feature_map = encoder(torch.cat((img_A, img_B), dim=1))\n",
    "    x_fake = generator(torch.cat((img_A, img_B, feature_map), dim=1))\n",
    "#     print(feature_map.shape)\n",
    "#     print(x_fake.shape)\n",
    "\n",
    "    # Get necessary outputs for loss/backprop for both generator and discriminator\n",
    "#     fake_preds_for_g = discriminator(x_fake)\n",
    "#     fake_preds_for_d = discriminator(x_fake.detach())\n",
    "#     real_preds_for_d = discriminator(x_real.detach())\n",
    "    fake_preds_for_g = discriminator(torch.cat((img_A, img_B, x_fake), dim=1))\n",
    "    fake_preds_for_d = discriminator(torch.cat((img_A, img_B, x_fake.detach()), dim=1))\n",
    "    real_preds_for_d = discriminator(torch.cat((img_A, img_B, x_real.detach()), dim=1))\n",
    "\n",
    "    g_loss = (\n",
    "        lambda0 * adv_loss(fake_preds_for_g, False) + \\\n",
    "        lambda1 * fm_loss(real_preds_for_d, fake_preds_for_g) / discriminator.n_discriminators + \\\n",
    "        lambda2 * vgg_loss(x_fake, x_real)  + \\\n",
    "#         1.0 * enc_loss(feature_map, img_B) + \\\n",
    "        2.0 * lpips_loss(x_fake, img_AtoB)\n",
    "    )\n",
    "\n",
    "    d_loss = 0.5 * (\n",
    "        adv_loss(real_preds_for_d, True) + \\\n",
    "        adv_loss(fake_preds_for_d, False)\n",
    "    )\n",
    "\n",
    "    return g_loss, d_loss, x_fake.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3cc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(0, 2)):\n",
    "    # training epoch\n",
    "    #\n",
    "    mean_g_loss = 0.0\n",
    "    mean_d_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    if not args.high_res:\n",
    "        encoder.train()\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    pbar = tqdm(dataloader, position=0, desc='train [G loss: -.----][D loss: -.----]')\n",
    "    for batch in pbar:\n",
    "        img_A, img_B, img_AtoB, img_BtoA = batch\n",
    "        img_A = img_A.to(device)\n",
    "        img_B = img_B.to(device)\n",
    "        img_AtoB = img_AtoB.to(device)\n",
    "        img_BtoA = img_BtoA.to(device)\n",
    "        \n",
    "        g_loss, d_loss, x_fake = forward_loss(\n",
    "            img_A,\n",
    "            img_B,\n",
    "            img_AtoB,\n",
    "            encoder,\n",
    "            generator,\n",
    "            discriminator\n",
    "        )\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        mean_g_loss += g_loss.item()\n",
    "        mean_d_loss += d_loss.item()\n",
    "        epoch_steps += 1\n",
    "\n",
    "        pbar.set_description(desc=f'train [G loss: {mean_g_loss/epoch_steps:.4f}][D loss: {mean_d_loss/epoch_steps:.4f}]')\n",
    "\n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57080e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.high_res:\n",
    "    encoder.eval()\n",
    "generator.eval()\n",
    "discriminator.eval()\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    img_A, img_B, img_AtoB, img_BtoA = batch\n",
    "    img_A = img_A.to(device)\n",
    "    img_B = img_B.to(device)\n",
    "    img_AtoB = img_AtoB.to(device)\n",
    "    img_BtoA = img_BtoA.to(device)\n",
    "    \n",
    "    if i==2: break\n",
    "    \n",
    "with torch.no_grad():\n",
    "#     feature_map = encoder(img_B)\n",
    "#     x_fake = generator(torch.cat((img_A, feature_map), dim=1))\n",
    "    feature_map = encoder(torch.cat((img_A, img_B), dim=1))\n",
    "    x_fake = generator(torch.cat((img_A, img_B, feature_map), dim=1))\n",
    "    \n",
    "\n",
    "show_tensor_images(img_A)\n",
    "show_tensor_images(img_B)\n",
    "show_tensor_images(img_AtoB)\n",
    "show_tensor_images(img_BtoA)\n",
    "show_tensor_images(x_fake)\n",
    "show_tensor_images(feature_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.0 * lpips_loss(x_fake, img_AtoB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57ea76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3570c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize logging\n",
    "loss = Pix2PixHDLoss(device=device)\n",
    "# log_dir = os.path.join(train_config.log_dir, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "# os.makedirs(log_dir, mode=0o775, exist_ok=False)\n",
    "\n",
    "for epoch in range(start_epoch, config.epochs):\n",
    "    # training epoch\n",
    "    mean_g_loss = 0.0\n",
    "    mean_d_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    if not high_res:\n",
    "        encoder.train()\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    pbar = tqdm(train_dataloader, position=0, desc='train [G loss: -.----][D loss: -.----]')\n",
    "    for (x_real, labels, insts, bounds) in pbar:\n",
    "        x_real = x_real.to(device)\n",
    "        labels = labels.to(device)\n",
    "        insts = insts.to(device)\n",
    "        bounds = bounds.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "            g_loss, d_loss, x_fake = loss(\n",
    "                x_real, labels, insts, bounds, encoder, generator, discriminator,\n",
    "            )\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        mean_g_loss += g_loss.item()\n",
    "        mean_d_loss += d_loss.item()\n",
    "        epoch_steps += 1\n",
    "\n",
    "        pbar.set_description(desc=f'train [G loss: {mean_g_loss/epoch_steps:.4f}][D loss: {mean_d_loss/epoch_steps:.4f}]')\n",
    "\n",
    "    if epoch+1 % train_config.save_every == 0:\n",
    "        torch.save({\n",
    "            'e_model_dict': encoder.state_dict(),\n",
    "            'g_model_dict': generator.state_dict(),\n",
    "            'd_model_dict': discriminator.state_dict(),\n",
    "            'g_optim_dict': g_optimizer.state_dict(),\n",
    "            'd_optim_dict': d_optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }, os.path.join(log_dir, f'epoch={epoch}.pt'))\n",
    "\n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7787d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
