# The MIT License
#
# Copyright (c) 2020 Vincent Liu
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

import os
from collections.abc import Iterable
import glob

import torch
import numpy as np
import torchvision.transforms as transforms
from PIL import Image


def scale_width(img, target_width, method):
    ''' Scales an image to target_width while retaining aspect ratio '''
    w, h = img.size
    if w == target_width: return img
    target_height = target_width * h // w
    return img.resize((target_width, target_height), method)


class CityscapesDataset(torch.utils.data.Dataset):
    ''' Implements dataset with preprocessing for Cityscapes dataset '''

    def __init__(self, paths, target_width=1024, n_classes=35):
        super().__init__()

        self.n_classes = n_classes

        # Collect list of examples
        self.examples = {}
        if type(paths) == str:
            self.load_examples_from_dir(paths)
        elif isinstance(paths, Iterable):
            for path in paths:
                self.load_examples_from_dir(path)
        else:
            raise ValueError('`paths` should be a single path or iterable of paths')

        self.examples = list(self.examples.values())
        examples = []
        n_unpaired = 0
        for example in self.examples:
            if len(example) == 3:
                examples.append(example)
            else:
                n_unpaired += 1

        self.examples = examples
        if n_unpaired > 0:
            print(f'{n_unpaired} examples found in dataset and will be removed')

        # Initialize transforms for the real color image
        self.img_transforms = transforms.Compose([
            transforms.Lambda(lambda img: scale_width(img, target_width, Image.BICUBIC)),
            transforms.Lambda(lambda img: np.array(img)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

        # Initialize transforms for semantic label and instance maps
        self.map_transforms = transforms.Compose([
            transforms.Lambda(lambda img: scale_width(img, target_width, Image.NEAREST)),
            transforms.Lambda(lambda img: np.array(img)),
            transforms.ToTensor(),
        ])

    def __getitem__(self, idx: int):
        example = self.examples[idx]

        # Load image and maps
        img = Image.open(example['orig_img']).convert('RGB') # color image: (3, 512, 1024)
        inst = Image.open(example['inst_map'])               # instance map: (512, 1024)
        label = Image.open(example['label_map'])             # semantic label map: (512, 1024)

        # Apply corresponding transforms
        img = self.img_transforms(img)
        inst = self.map_transforms(inst)
        label = self.map_transforms(label).long() * 255

        # Convert labels to one-hot vectors
        label = torch.zeros(self.n_classes, img.shape[1], img.shape[2]).scatter_(0, label, 1.0).to(img.dtype)

        # Convert instance map to instance boundary map
        bound = torch.ByteTensor(inst.shape).zero_()
        bound[:, :, 1:] = bound[:, :, 1:] | (inst[:, :, 1:] != inst[:, :, :-1])
        bound[:, :, :-1] = bound[:, :, :-1] | (inst[:, :, 1:] != inst[:, :, :-1])
        bound[:, 1:, :] = bound[:, 1:, :] | (inst[:, 1:, :] != inst[:, :-1, :])
        bound[:, :-1, :] = bound[:, :-1, :] | (inst[:, 1:, :] != inst[:, :-1, :])
        bound = bound.to(img.dtype)

        return (img, label, inst, bound)

    def __len__(self):
        return len(self.examples)

    def load_examples_from_dir(self, abs_path):
        ''' Returns a list of paired examples given a folder '''
        assert os.path.isdir(abs_path)

        img_suffix = '_leftImg8bit.png'
        label_suffix = '_gtFine_labelIds.png'
        inst_suffix = '_gtFine_instanceIds.png'

        for root, _, files in os.walk(abs_path):
            for f in files:
                if f.endswith(img_suffix):
                    prefix = f[:-len(img_suffix)]
                    attr = 'orig_img'
                elif f.endswith(label_suffix):
                    prefix = f[:-len(label_suffix)]
                    attr = 'label_map'
                elif f.endswith(inst_suffix):
                    prefix = f[:-len(inst_suffix)]
                    attr = 'inst_map'
                else:
                    continue

                if prefix not in self.examples.keys():
                    self.examples[prefix] = {}
                self.examples[prefix][attr] = root + '/' + f

    @staticmethod
    def collate_fn(batch):
        imgs, labels, insts, bounds = [], [], [], []
        for (x, l, i, b) in batch:
            imgs.append(x)
            labels.append(l)
            insts.append(i)
            bounds.append(b)
        return (
            torch.stack(imgs, dim=0),
            torch.stack(labels, dim=0),
            torch.stack(insts, dim=0),
            torch.stack(bounds, dim=0),
        )

    
    
class StyleGANFaces(torch.utils.data.Dataset):
    '''
    Implements dataset with preprocessing for image-to-image dataset 
    generated from StyleGAN2.
    '''

    def __init__(self,
                 path_A,
                 path_B,
                 path_AtoB,
                 path_BtoA,
                 ext=".png",
                 target_width=256):
        super().__init__()
        
        self.path_A = path_A
        self.path_B = path_B
        self.path_AtoB = path_AtoB
        self.path_BtoA = path_BtoA

        self.files_A = sorted(glob.glob(f'{path_A}/*{ext}'))
        self.files_B = sorted(glob.glob(f'{path_B}/*{ext}'))
        self.files_AtoB = sorted(glob.glob(f'{path_AtoB}/*{ext}'))
        self.files_BtoA = sorted(glob.glob(f'{path_BtoA}/*{ext}'))
        assert len(self.files_A) == len(self.files_B) == len(self.files_AtoB) == len(self.files_BtoA)

        # Initialize transforms for the real color image
        self.img_transforms = transforms.Compose([
            transforms.Lambda(lambda img: scale_width(img, target_width, Image.BICUBIC)),
            transforms.Lambda(lambda img: np.array(img)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

        
    def __getitem__(self, idx: int):
        img_A = self.img_transforms(Image.open(self.files_A[idx]))
        img_B = self.img_transforms(Image.open(self.files_B[idx]))
        img_AtoB = self.img_transforms(Image.open(self.files_AtoB[idx]))
        img_BtoA = self.img_transforms(Image.open(self.files_BtoA[idx]))
        
        return (img_A, img_B, img_AtoB, img_BtoA)

    
    def __len__(self):
        '''All have been asserted to be equal length, so just return one'''
        return len(self.files_A)


#     @staticmethod
#     def collate_fn(batch):
#         imgs, labels, insts, bounds = [], [], [], []
#         for (x, l, i, b) in batch:
#             imgs.append(x)
#             labels.append(l)
#             insts.append(i)
#             bounds.append(b)
#         return (
#             torch.stack(imgs, dim=0),
#             torch.stack(labels, dim=0),
#             torch.stack(insts, dim=0),
#             torch.stack(bounds, dim=0),
#         )