# The MIT License
#
# Copyright (c) 2020 Vincent Liu
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

from collections.abc import Iterable

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models


class ResidualBlock(nn.Module):
    ''' Implements a residual block '''

    def __init__(self, channels: int):
        super().__init__()

        self.layers = nn.Sequential(
            nn.ReflectionPad2d(1),
            nn.Conv2d(channels, channels, kernel_size=3, padding=0),
            nn.InstanceNorm2d(channels, affine=False),

            nn.ReLU(inplace=True),

            nn.ReflectionPad2d(1),
            nn.Conv2d(channels, channels, kernel_size=3, padding=0),
            nn.InstanceNorm2d(channels, affine=False),
        )

    def forward(self, x):
        return x + self.layers(x)


class GlobalGenerator(nn.Module):
    ''' Implements the global subgenerator (G1) for transferring styles at lower resolutions '''

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        base_channels: int = 64,
        fb_blocks: int = 3,
        res_blocks: int = 9,
    ):
        super().__init__()

        # Initial convolutional layer
        g1 = [
            nn.ReflectionPad2d(3),
            nn.Conv2d(in_channels, base_channels, kernel_size=7, padding=0),
            nn.InstanceNorm2d(base_channels, affine=False),
            nn.ReLU(inplace=True),
        ]

        channels = base_channels
        # Frontend blocks
        for _ in range(fb_blocks):
            g1 += [
                nn.Conv2d(channels, 2 * channels, kernel_size=3, stride=2, padding=1),
                nn.InstanceNorm2d(2 * channels, affine=False),
                nn.ReLU(inplace=True),
            ]
            channels *= 2

        # Residual blocks
        for _ in range(res_blocks):
            g1 += [ResidualBlock(channels)]

        # Backend blocks
        for _ in range(fb_blocks):
            g1 += [
                nn.ConvTranspose2d(channels, channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1),
                nn.InstanceNorm2d(channels // 2, affine=False),
                nn.ReLU(inplace=True),
            ]
            channels //= 2

        # Output convolutional layer as its own nn.Sequential since it will be omitted in second training phase
        self.out_layers = nn.Sequential(
            nn.ReflectionPad2d(3),
            nn.Conv2d(base_channels, out_channels, kernel_size=7, padding=0),
            nn.Tanh(),
        )

        self.g1 = nn.Sequential(*g1)

    def forward(self, x):
        x = self.g1(x)
        x = self.out_layers(x)
        return x


class LocalEnhancer(nn.Module):
    ''' Implements the local enhancer subgenerator (G2) for handling larger scale images '''

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        base_channels: int = 32,
        global_fb_blocks: int = 3,
        global_res_blocks: int = 9,
        local_res_blocks: int = 3,
    ):
        super().__init__()

        global_base_channels = 2 * base_channels

        # Downsampling layer for high-res -> low-res input to g1
        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)

        # Initialize global generator without its input and output layers
        self.g1 = GlobalGenerator(
            in_channels, out_channels, base_channels=global_base_channels,
            fb_blocks=global_fb_blocks, res_blocks=global_res_blocks,
        ).g1

        self.g2 = nn.ModuleList()

        # Initialize local frontend block
        self.g2.append(
            nn.Sequential(
                # Initial convolutional layer
                nn.ReflectionPad2d(3),
                nn.Conv2d(in_channels, base_channels, kernel_size=7, padding=0), 
                nn.InstanceNorm2d(base_channels, affine=False),
                nn.ReLU(inplace=True),

                # Frontend block
                nn.Conv2d(base_channels, 2 * base_channels, kernel_size=3, stride=2, padding=1), 
                nn.InstanceNorm2d(2 * base_channels, affine=False),
                nn.ReLU(inplace=True),
            )
        )

        # Initialize local residual and backend blocks
        self.g2.append(
            nn.Sequential(
                # Residual blocks
                *[ResidualBlock(2 * base_channels) for _ in range(local_res_blocks)],

                # Backend blocks
                nn.ConvTranspose2d(2 * base_channels, base_channels,
                                   kernel_size=3, stride=2, padding=1, output_padding=1), 
                nn.InstanceNorm2d(base_channels, affine=False),
                nn.ReLU(inplace=True),

                # Output convolutional layer
                nn.ReflectionPad2d(3),
                nn.Conv2d(base_channels, out_channels, kernel_size=7, padding=0),
                nn.Tanh(),
            )
        )

    def forward(self, x):
        # Get output from g1_B
        x_g1 = self.downsample(x)
        x_g1 = self.g1(x_g1)

        # Get output from g2_F
        x_g2 = self.g2[0](x)

        # Get final output from g2_B
        return self.g2[1](x_g1 + x_g2)


class Discriminator(nn.Module):
    ''' Implements a PatchGAN discriminator, which can be used for all the different scales '''

    def __init__(
        self,
        in_channels: int,
        base_channels: int = 64,
        n_layers: int = 3,
    ):
        super().__init__()

        # Use nn.ModuleList so we can output intermediate values for loss.
        self.layers = nn.ModuleList()

        # Initial convolutional layer
        self.layers.append(
            nn.Sequential(
                nn.Conv2d(in_channels, base_channels, kernel_size=4, stride=2, padding=2),
                nn.LeakyReLU(0.2, inplace=True),
            )
        )

        # Downsampling convolutional layers
        channels = base_channels
        for _ in range(1, n_layers):
            prev_channels = channels
            channels = min(2 * channels, 512)
            self.layers.append(
                nn.Sequential(
                    nn.Conv2d(prev_channels, channels, kernel_size=4, stride=2, padding=2),
                    nn.InstanceNorm2d(channels, affine=False),
                    nn.LeakyReLU(0.2, inplace=True),
                )
            )

        # Output convolutional layer
        prev_channels = channels
        channels = min(2 * channels, 512)
        self.layers.append(
            nn.Sequential(
                nn.Conv2d(prev_channels, channels, kernel_size=4, stride=1, padding=2),
                nn.InstanceNorm2d(channels, affine=False),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(channels, 1, kernel_size=4, stride=1, padding=2),
            )
        )

    def forward(self, x):
        outputs = [] # for feature matching loss
        for layer in self.layers:
            x = layer(x)
            outputs.append(x)

        return outputs


class MultiscaleDiscriminator(nn.Module):
    ''' Implements a multiscale discriminator for different resolutions '''

    def __init__(
        self,
        in_channels: int,
        base_channels: int = 64,
        n_layers: int = 3,
        n_discriminators: int = 3,
    ):
        super().__init__()

        # Initialize all discriminators
        self.discriminators = nn.ModuleList()
        for _ in range(n_discriminators):
            self.discriminators.append(
                Discriminator(in_channels, base_channels=base_channels, n_layers=n_layers)
            )

        # Downsampling layer to pass inputs between discriminators at different scales
        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)

    def forward(self, x):
        outputs = []

        for i, discriminator in enumerate(self.discriminators):
            # Downsample input for subsequent discriminators
            if i != 0:
                x = self.downsample(x)

            outputs.append(discriminator(x))

        # Return list of multiscale discriminator outputs
        return outputs

    @property
    def n_discriminators(self):
        return len(self.discriminators)


# class Encoder(nn.Module):
#     ''' Implements an encoder with instance-wise average pooling for feature mapping '''

#     def __init__(
#         self,
#         in_channels: int,
#         out_channels: int,
#         base_channels: int = 16,
#         n_layers: int = 4,
#     ):
#         super().__init__()

#         self.out_channels = out_channels
#         channels = base_channels

#         layers = [
#             nn.ReflectionPad2d(3),
#             nn.Conv2d(in_channels, base_channels, kernel_size=7, padding=0), 
#             nn.InstanceNorm2d(base_channels),
#             nn.ReLU(inplace=True),
#         ]

#         # Downsampling layers
#         for i in range(n_layers):
#             layers += [
#                 nn.Conv2d(channels, 2 * channels, kernel_size=3, stride=2, padding=1),
#                 nn.InstanceNorm2d(2 * channels),
#                 nn.ReLU(inplace=True),
#             ]
#             channels *= 2
    
#         # Upsampling layers
#         for i in range(n_layers):
#             layers += [
#                 nn.ConvTranspose2d(channels, channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1),
#                 nn.InstanceNorm2d(channels // 2),
#                 nn.ReLU(inplace=True),
#             ]
#             channels //= 2

#         layers += [
#             nn.ReflectionPad2d(3),
#             nn.Conv2d(base_channels, out_channels, kernel_size=7, padding=0),
#             nn.Tanh(),
#         ]

#         self.layers = nn.Sequential(*layers)

#     def instancewise_average_pooling(self, x, inst):
#         '''
#         Applies instance-wise average pooling.

#         Given a feature map of size (b, c, h, w), the mean is computed for each b, c
#         across all h, w of the same instance
#         '''
#         x_mean = torch.zeros_like(x)
#         classes = torch.unique(inst, return_inverse=False, return_counts=False) # gather all unique classes present

#         for i in classes:
#             for b in range(x.size(0)):
#                 indices = torch.nonzero(inst[b:b+1] == i, as_tuple=False) # get indices of all positions equal to class i
#                 for j in range(self.out_channels):
#                     x_ins = x[indices[:, 0] + b, indices[:, 1] + j, indices[:, 2], indices[:, 3]]
#                     mean_feat = torch.mean(x_ins).expand_as(x_ins)
#                     x_mean[indices[:, 0] + b, indices[:, 1] + j, indices[:, 2], indices[:, 3]] = mean_feat

#         return x_mean

#     def forward(self, x, inst=None):
#         x = self.layers(x)
#         if inst:
#             x = self.instancewise_average_pooling(x, inst)
#         return x


# class Encoder(nn.Module):
#     ''' Implements an encoder with instance-wise average pooling for feature mapping '''

#     def __init__(
#         self,
#         in_channels: int,
#         out_channels: int,
#         base_channels: int = 16,
#         n_layers: int = 4,
#     ):
#         super().__init__()

#         self.out_channels = out_channels
#         channels = base_channels

#         layers = [
#             nn.ReflectionPad2d(3),
#             nn.Conv2d(in_channels, base_channels, kernel_size=7, padding=0), 
#             nn.InstanceNorm2d(base_channels),
#             nn.ReLU(inplace=True),
#         ]

#         # Downsampling layers
#         for i in range(n_layers):
#             layers += [
#                 nn.Conv2d(channels, channels * 2, kernel_size=3, stride=2, padding=1),
#                 nn.InstanceNorm2d(channels * 2),
#                 nn.ReLU(inplace=True),
#             ]
#             channels *= 2
    
#         # Upsampling layers
#         for i in range(n_layers):
#             layers += [
#                 nn.ConvTranspose2d(channels, channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1),
#                 nn.InstanceNorm2d(channels // 2),
#                 nn.ReLU(inplace=True),
#             ]
#             channels //= 2

#         layers += [
#             nn.ReflectionPad2d(3),
#             nn.Conv2d(base_channels, out_channels, kernel_size=7, padding=0),
# #             nn.Tanh(),
# #             nn.ReLU(inplace=True),
# #             nn.Sigmoid(),
#         ]

#         self.layers = nn.Sequential(*layers)

#     def forward(self, x, inst=None):
#         x = self.layers(x)
#         x = torch.clamp(x, -1., 1.)
#         return x
    

import lpips
class Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.lpips = lpips.LPIPS(net='vgg', spatial=True)
        
    def forward(self, x):
        a, b = torch.chunk(x, 2, dim=1)
        res = self.lpips(a, b).repeat(1,3,1,1) # [N,1,H,W] -> [N,3,H,W]
#         res = torch.clamp(res, -1., 1.)
        res = nn.Sigmoid()(res)
        return res
    
    

class VGG19(nn.Module):
    ''' Wrapper for pretrained torchvision.models.vgg19 to output intermediate feature maps '''

    def __init__(self):
        super().__init__()

        vgg_features = models.vgg19(pretrained=True).features

        self.f1 = nn.Sequential(*[vgg_features[x] for x in range(2)])
        self.f2 = nn.Sequential(*[vgg_features[x] for x in range(2, 7)])
        self.f3 = nn.Sequential(*[vgg_features[x] for x in range(7, 12)])
        self.f4 = nn.Sequential(*[vgg_features[x] for x in range(12, 21)])
        self.f5 = nn.Sequential(*[vgg_features[x] for x in range(21, 30)])

        for param in self.parameters():
            param.requires_grad = False

    def forward(self, x):
        h1 = self.f1(x)
        h2 = self.f2(h1)
        h3 = self.f3(h2)
        h4 = self.f4(h3)
        h5 = self.f5(h4)
        return [h1, h2, h3, h4, h5]
